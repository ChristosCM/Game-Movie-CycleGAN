{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Game-Movie.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "shB48aYlwlgi",
        "JzHFRBgLW72B",
        "SzqOoDeO7GrD",
        "2n7Tq7krjxa-"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBVpHIxtwjdS"
      },
      "source": [
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import os\n",
        "import os.path\n",
        "import glob\n",
        "import random\n",
        "import cv2\n",
        "import subprocess\n",
        "!pip install -q --upgrade wandb==0.10.8\n",
        "import wandb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "import matplotlib.pyplot as plt\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)\n",
        "import itertools\n",
        "import torchvision.utils as vutils\n",
        "from torch.nn import init\n",
        "import h5py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shB48aYlwlgi"
      },
      "source": [
        "# 1.1 Preprocessing and Extraction of frames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clZNt3qDwsA0"
      },
      "source": [
        "add the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Xnime6nw4Ee",
        "outputId": "c08b828d-fa2e-4295-ed08-e0f1b602df29"
      },
      "source": [
        "#get the names of the files\n",
        "movie_files = []\n",
        "video_dataset = \"/content/drive/MyDrive/Game-Movie/video_data\"\n",
        "for dirname, _, filenames in os.walk(video_dataset):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "        if filename!=\"MafiaVideogame.mp4\":\n",
        "            movie_files.append(os.path.join(dirname, filename))\n",
        "        else:\n",
        "            game_file = os.path.join(dirname, filename)\n",
        "\n",
        "def get_length(filename):\n",
        "    result = subprocess.run([\"ffprobe\", \"-v\", \"error\", \"-show_entries\",\n",
        "                             \"format=duration\", \"-of\",\n",
        "                             \"default=noprint_wrappers=1:nokey=1\", filename],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT)\n",
        "    return float(result.stdout)\n",
        "\n",
        "def get_secs(filename):\n",
        "    video = cv2.VideoCapture(filename) \n",
        "    \n",
        "    # count the number of frames \n",
        "    frames = video.get(cv2.CAP_PROP_FRAME_COUNT) \n",
        "    fps = int(video.get(cv2.CAP_PROP_FPS)) \n",
        "\n",
        "    seconds = int(frames / fps) \n",
        "    return seconds\n",
        "def get_name(filename): #get just the name of the file for naming purposes\n",
        "    name = filename.split(\".\")[0]\n",
        "    name = name.split(\"/\")[-1]\n",
        "    return name\n",
        "\n",
        "def get_frames(filename,amount,new_path):\n",
        "    count=0\n",
        "    Path(new_path).mkdir(parents=True, exist_ok=True)\n",
        "    name = get_name(filename)\n",
        "    total_secs = get_secs(filename)\n",
        "    \n",
        "    video = cv2.VideoCapture(filename)   # capturing the video from the given path\n",
        "    frameRate = int(video.get(cv2.CAP_PROP_FPS)) #frame rate\n",
        "    distance = (total_secs*frameRate)//amount #get a frame every distance seconds so that they are uniformly spaced out from each other\n",
        "    while (video.isOpened()):\n",
        "        frameID = video.get(1) #get the number of the current frame\n",
        "        \n",
        "        ret, frame = video.read()\n",
        "        \n",
        "        if (ret!=True):\n",
        "            break\n",
        "        if (frameID%distance==0):\n",
        "            new_file = new_path+name+\"_\"+str(count)+\".jpg\"\n",
        "            count+=1\n",
        "            cv2.imwrite(new_file,frame)\n",
        "    video.release()\n",
        "    print(\"Frames extracted\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Game-Movie/video_data/game/MafiaVideogame.mp4\n",
            "/content/drive/MyDrive/Game-Movie/video_data/movie/TheIrishman.mp4\n",
            "/content/drive/MyDrive/Game-Movie/video_data/movie/TheSopranos.mp4\n",
            "/content/drive/MyDrive/Game-Movie/video_data/movie/TheGodfather.mp4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHWhcWl6xxlU",
        "outputId": "ba3b7598-bd7e-4d53-af2c-19f2961307d3"
      },
      "source": [
        "get_frames(game_file,3000,\"/content/drive/MyDrive/Game-Movie/images/game/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Frames extracted\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKCY2062ycdr",
        "outputId": "7f58f1e1-1d0c-44db-eb1f-c1ea5f30b899"
      },
      "source": [
        "for movie in movie_files:\n",
        "    get_frames(movie,1000,\"/content/drive/MyDrive/Game-Movie/images/movies/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Frames extracted\n",
            "Frames extracted\n",
            "Frames extracted\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RTXG1cW6Df7",
        "outputId": "0a93e07d-0c70-4007-85af-a99f9dc18e47"
      },
      "source": [
        "#resize the images to 256x256\n",
        "game_path = \"/content/drive/MyDrive/Game-Movie/images/game/\" #path for games and images\n",
        "movies_path = \"/content/drive/MyDrive/Game-Movie/images/movies/\"\n",
        "game_images = next(os.walk(game_path))[2]\n",
        "movie_images = next(os.walk(movies_path))[2]\n",
        "\n",
        "def fullNames(path,files):\n",
        "    files = [path+x for x in files]\n",
        "    return files\n",
        "game_images = fullNames(game_path,game_images)\n",
        "movie_images = fullNames(movies_path,movie_images)\n",
        "def cropNsize(files,out_path):\n",
        "    Path(out_path).mkdir(parents=True, exist_ok=True)\n",
        "    for image in files:\n",
        "        im = Image.open(image)\n",
        "        name = image.split(\"/\")[-1]\n",
        "        im = im.crop((108, 0, 1173, 600)) \n",
        "        im = im.resize((256,256), Image.ANTIALIAS)\n",
        "        im.save(out_path+name, 'JPEG', quality=90)\n",
        "    print(\"All Images in folder resized\")\n",
        "cropNsize(game_images,\"/content/drive/MyDrive/Game-Movie/images256/game/\")\n",
        "cropNsize(movie_images,\"/content/drive/MyDrive/Game-Movie/images256/movies/\")\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All Images in folder resized\n",
            "All Images in folder resized\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzHFRBgLW72B"
      },
      "source": [
        "#1.1.1 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P5sHt6w5W7Q"
      },
      "source": [
        "def get_files(path):\n",
        "    files = []\n",
        "    for dirname, _, filenames in os.walk(path):\n",
        "        for filename in filenames:\n",
        "            files.append(os.path.join(dirname, filename))\n",
        "    return files\n",
        "            \n",
        "class dataset(Dataset):\n",
        "    def __init__(self, path,transform=None):\n",
        "        self.transform = transform\n",
        "        self.files = get_files(path)\n",
        "        self.length = len(self.files)\n",
        "    def __getitem__(self, index):\n",
        "        image = Image.open(self.files[index])\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "game_path = \"/content/drive/MyDrive/Game-Movie/images256/game/\"\n",
        "movies_path = \"/content/drive/MyDrive/Game-Movie/images256/movies/\"\n",
        "\n",
        "trans = transforms.Compose([transforms.ToTensor(),])\n",
        "\n",
        "all_game = dataset(path=game_path,transform=trans)\n",
        "all_movie = dataset(path=movies_path,transform = trans)\n",
        "\n",
        "game_train_dataset, game_test_dataset, game_validation_dataset = random_split(all_game,[2000,500,len(all_game)-2500], generator=torch.Generator().manual_seed(42))\n",
        "movie_train_dataset, movie_test_dataset, movie_validation_dataset = random_split(all_movie,[2000,500,len(all_movie)-2500], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "batch_size=4\n",
        "game_train_loader = DataLoader(game_train_dataset,batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "movie_train_loader = DataLoader(movie_train_dataset,batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "\n",
        "Path(\"images/\").mkdir(parents=True, exist_ok=True) #generate path to save produced images\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZKKakvvyrgS"
      },
      "source": [
        "# 2.1 Frame-to-Frame Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzqOoDeO7GrD"
      },
      "source": [
        "## 2.1.1 ResNet Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3B7rZR__yqP1"
      },
      "source": [
        "#code is adapted from: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py\n",
        "use_bias=True\n",
        "use_dropout=True\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self,norm=\"instance\"):\n",
        "        super(Generator, self).__init__()\n",
        "        #initial layer\n",
        "        if norm==\"instance\":\n",
        "            normalization = nn.InstanceNorm2d\n",
        "        elif norm==\"batch\":\n",
        "            normalization = nn.BatchNorm2d\n",
        "        self.pad = nn.ReflectionPad2d(3)\n",
        "        self.conv1 = nn.Conv2d(3,64,7,padding=0,bias=use_bias)\n",
        "        self.batch64 = normalization(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        \n",
        "        #downsampling layers\n",
        "        self.conv2 = nn.Conv2d(64, 128, 3, stride=2, padding=1,bias=use_bias)\n",
        "        self.batch128 = normalization(128)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(128, 256, 3, stride=2, padding=1,bias=use_bias)\n",
        "        self.batch256 = normalization(256)\n",
        "        \n",
        "        #Residual blocks\n",
        "        self.resblock1 = ResidualBlock(256,norm=\"batch\")\n",
        "        self.resblock2 = ResidualBlock(256,norm=\"batch\")\n",
        "        self.resblock3 = ResidualBlock(256,norm=\"batch\")\n",
        "        self.resblock4 = ResidualBlock(256,norm=\"batch\")\n",
        "        self.resblock5 = ResidualBlock(256,norm=\"batch\")\n",
        "        self.resblock6 = ResidualBlock(256,norm=\"batch\")\n",
        "        self.resblock7 = ResidualBlock(256,norm=\"batch\")\n",
        "        self.resblock8 = ResidualBlock(256,norm=\"batch\")\n",
        "        self.resblock9 = ResidualBlock(256,norm=\"batch\")\n",
        "        \n",
        "        #upsample\n",
        "        self.convtrans1 = nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1,bias=use_bias)\n",
        "        self.convtrans2 = nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1,bias=use_bias)\n",
        "        #or to avoid artifacts:\n",
        "        self.upsample1 = nn.Sequential(nn.Upsample(scale_factor = 2, mode='bilinear', align_corners=False), \n",
        "                      nn.ReflectionPad2d(1),                                               \n",
        "                      nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=0, bias=use_bias))\n",
        "        self.upsample2 = nn.Sequential(nn.Upsample(scale_factor = 2, mode='bilinear', align_corners=False), \n",
        "                      nn.ReflectionPad2d(1),                                               \n",
        "                      nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=0, bias=use_bias))\n",
        "        #output\n",
        "        self.outconv = nn.Conv2d(64,3,7,padding=0)\n",
        "        self.out = nn.Tanh()\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x = self.pad(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(self.batch64(x))\n",
        "        \n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(self.batch128(x))\n",
        "        \n",
        "        x = self.conv3(x)\n",
        "        x = self.relu(self.batch256(x))\n",
        "        \n",
        "        x = self.resblock1(x)\n",
        "        x = self.resblock2(x)\n",
        "        x = self.resblock3(x)\n",
        "        x = self.resblock4(x)\n",
        "        x = self.resblock5(x)\n",
        "        x = self.resblock6(x)\n",
        "        x = self.resblock7(x)\n",
        "        x = self.resblock8(x)\n",
        "        x = self.resblock9(x)\n",
        "\n",
        "        x = self.convtrans1(x)\n",
        "        x = self.relu(self.batch128(x))\n",
        "        x = self.convtrans2(x)\n",
        "        x = self.relu(self.batch64(x))\n",
        "        \n",
        "        \n",
        "        # x = self.upsample1(x)\n",
        "        # x = self.relu(self.batch128(x))\n",
        "        # x = self.upsample2(x)\n",
        "        # x = self.relu(self.batch64(x))\n",
        "\n",
        "        x = self.pad(x)\n",
        "        x = self.outconv(x)\n",
        "        return self.out(x)\n",
        "    \n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels,norm=\"instance\"):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        if norm==\"instance\":\n",
        "            normalization = nn.InstanceNorm2d\n",
        "        elif norm==\"batch\":\n",
        "            normalization = nn.BatchNorm2d\n",
        "        if use_dropout:\n",
        "            self.res = nn.Sequential(nn.ReflectionPad2d(1),\n",
        "                                 nn.Conv2d(in_channels, in_channels, 3,padding=0,bias=use_bias),\n",
        "                                 normalization(in_channels),\n",
        "                                 nn.ReLU(inplace=True),\n",
        "                                 nn.Dropout(0.5),\n",
        "                                 nn.ReflectionPad2d(1),\n",
        "                                 nn.Conv2d(in_channels, in_channels, 3,padding=0,bias=use_bias),\n",
        "                                 normalization(in_channels))\n",
        "        else:\n",
        "            self.res = nn.Sequential(nn.ReflectionPad2d(1),\n",
        "                                 nn.Conv2d(in_channels, in_channels, 3,padding=0,bias=use_bias),\n",
        "                                 nn.normalization(in_channels),\n",
        "                                 nn.ReLU(inplace=True),\n",
        "                                 nn.ReflectionPad2d(1),\n",
        "                                 nn.Conv2d(in_channels, in_channels, 3,padding=0,bias=use_bias),\n",
        "                                 nn.normalization(in_channels))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.res(x)\n",
        "gtm = Generator(norm=\"batch\").to(device)\n",
        "mtg = Generator(norm=\"batch\").to(device)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE0iIOII7MMw"
      },
      "source": [
        "## 2.1.2 Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Os4PDjfC7EHM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff028ce1-36d3-4082-f551-60ae818b62b8"
      },
      "source": [
        "class NLayerDiscriminator(nn.Module):\n",
        "    def __init__(self, norm=\"instance\"):\n",
        "        super(NLayerDiscriminator, self).__init__()\n",
        "\n",
        "        if norm==\"instance\":\n",
        "            normalization = nn.InstanceNorm2d\n",
        "        elif norm==\"batch\":\n",
        "            normalization = nn.BatchNorm2d\n",
        "        use_bias=False\n",
        "        self.conv1 = nn.Conv2d(3,64,4,stride=2,padding=1)\n",
        "        self.conv2 = nn.Conv2d(64,128,4,stride=2,padding=1,bias=use_bias)\n",
        "        self.norm2 = normalization(128)\n",
        "        self.conv3 = nn.Conv2d(128,256,4,stride=2,padding=1,bias=use_bias)\n",
        "        self.norm3 = normalization(256)\n",
        "        self.conv4 = nn.Conv2d(256,512,4,stride=2,padding=1,bias=use_bias)\n",
        "        self.norm4 = normalization(512)\n",
        "        self.conv5 = nn.Conv2d(512,1,4,stride=1,padding=1,bias=use_bias)\n",
        "        self.out = nn.Sigmoid() \n",
        "        self.leakyrelu = nn.LeakyReLU(0.2, True)\n",
        "        self.pad = nn.ZeroPad2d((1,0,1,0))\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.leakyrelu(self.conv1(x))\n",
        "        x = self.leakyrelu(self.norm2(self.conv2(x)))\n",
        "        x = self.leakyrelu(self.norm3(self.conv3(x)))\n",
        "        x = self.leakyrelu(self.norm4(self.conv4(x)))\n",
        "        x = self.pad(x)\n",
        "        x = self.conv5(x)\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "D_game = NLayerDiscriminator(norm=\"batch\").to(device)\n",
        "D_movie = NLayerDiscriminator(norm=\"batch\").to(device)\n",
        "summary(D_game,(3,256,256))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 128, 128]           3,136\n",
            "         LeakyReLU-2         [-1, 64, 128, 128]               0\n",
            "            Conv2d-3          [-1, 128, 64, 64]         131,072\n",
            "       BatchNorm2d-4          [-1, 128, 64, 64]             256\n",
            "         LeakyReLU-5          [-1, 128, 64, 64]               0\n",
            "            Conv2d-6          [-1, 256, 32, 32]         524,288\n",
            "       BatchNorm2d-7          [-1, 256, 32, 32]             512\n",
            "         LeakyReLU-8          [-1, 256, 32, 32]               0\n",
            "            Conv2d-9          [-1, 512, 16, 16]       2,097,152\n",
            "      BatchNorm2d-10          [-1, 512, 16, 16]           1,024\n",
            "        LeakyReLU-11          [-1, 512, 16, 16]               0\n",
            "        ZeroPad2d-12          [-1, 512, 17, 17]               0\n",
            "           Conv2d-13            [-1, 1, 16, 16]           8,192\n",
            "          Sigmoid-14            [-1, 1, 16, 16]               0\n",
            "================================================================\n",
            "Total params: 2,765,632\n",
            "Trainable params: 2,765,632\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.75\n",
            "Forward/backward pass size (MB): 38.13\n",
            "Params size (MB): 10.55\n",
            "Estimated Total Size (MB): 49.43\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UyQ4eIwR_po"
      },
      "source": [
        "class BranchDiscriminator(nn.Module):\n",
        "    def __init__(self,nlayer=True,pixel=True,norm = \"instance\"):\n",
        "        super(BranchDiscriminator,self).__init__()\n",
        "        if norm==\"instance\":\n",
        "            normalization = nn.InstanceNorm2d\n",
        "        elif norm==\"batch\":\n",
        "            normalization = nn.BatchNorm2d\n",
        "        use_bias=True\n",
        "        self.nlayer=nn.Sequential(\n",
        "            nn.Conv2d(3,64,4,stride=2,padding=1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(64,128,4,stride=2,padding=1,bias=use_bias),\n",
        "            normalization(128),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(128,256,4,stride=2,padding=1,bias=use_bias),\n",
        "            normalization(256),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(256,512,4,stride=2,padding=1,bias=use_bias),\n",
        "            normalization(512),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
        "            nn.Conv2d(512,1,4,stride=1,padding=1,bias=use_bias),\n",
        "        )\n",
        "        self.pixel=nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=1, stride=1, padding=0),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(64, 128, kernel_size=1, stride=1, padding=0),\n",
        "            normalization(128),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(128, 1, kernel_size=1, stride=1, padding=0),\n",
        "            nn.MaxPool2d(16,16)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self,x):\n",
        "        x_layer = self.nlayer(x)\n",
        "        x_pixel = self.pixel(x)\n",
        "        # print(x_pixel.shape)\n",
        "        # x_layer_sigmoid =  self.sigmoid(x_layer)\n",
        "        # x_pixel_sigmoid = self.sigmoid(x_pixel)\n",
        "        # x_combined = self.sigmoid(x_layer_sigmoid+x_pixel_sigmoid)\n",
        "        x_combined = self.sigmoid(x_layer+x_pixel)\n",
        "        return x_combined\n",
        "D_game = BranchDiscriminator(norm=\"batch\").to(device)\n",
        "D_movie = BranchDiscriminator(norm=\"batch\").to(device)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzGR_BO3TvXZ",
        "outputId": "71a2ef78-7e1b-4a90-b8fb-b7468d56facb"
      },
      "source": [
        "summary(D_game,(3,256,256))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 128, 128]           3,136\n",
            "         LeakyReLU-2         [-1, 64, 128, 128]               0\n",
            "            Conv2d-3          [-1, 128, 64, 64]         131,200\n",
            "       BatchNorm2d-4          [-1, 128, 64, 64]             256\n",
            "         LeakyReLU-5          [-1, 128, 64, 64]               0\n",
            "            Conv2d-6          [-1, 256, 32, 32]         524,544\n",
            "       BatchNorm2d-7          [-1, 256, 32, 32]             512\n",
            "         LeakyReLU-8          [-1, 256, 32, 32]               0\n",
            "            Conv2d-9          [-1, 512, 16, 16]       2,097,664\n",
            "      BatchNorm2d-10          [-1, 512, 16, 16]           1,024\n",
            "        LeakyReLU-11          [-1, 512, 16, 16]               0\n",
            "        ZeroPad2d-12          [-1, 512, 17, 17]               0\n",
            "           Conv2d-13            [-1, 1, 16, 16]           8,193\n",
            "           Conv2d-14         [-1, 64, 256, 256]             256\n",
            "        LeakyReLU-15         [-1, 64, 256, 256]               0\n",
            "           Conv2d-16        [-1, 128, 256, 256]           8,320\n",
            "      BatchNorm2d-17        [-1, 128, 256, 256]             256\n",
            "        LeakyReLU-18        [-1, 128, 256, 256]               0\n",
            "           Conv2d-19          [-1, 1, 256, 256]             129\n",
            "        MaxPool2d-20            [-1, 1, 16, 16]               0\n",
            "          Sigmoid-21            [-1, 1, 16, 16]               0\n",
            "================================================================\n",
            "Total params: 2,775,490\n",
            "Trainable params: 2,775,490\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.75\n",
            "Forward/backward pass size (MB): 294.63\n",
            "Params size (MB): 10.59\n",
            "Estimated Total Size (MB): 305.97\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJvy83WP7WO9",
        "outputId": "792efc13-91e5-4ae0-c948-d9ab8de4cfed"
      },
      "source": [
        "def init_weights(net, init_type='normal', init_gain=0.02):\n",
        "    def init_func(m):  # define the initialization function\n",
        "        classname = m.__class__.__name__\n",
        "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
        "            if init_type == 'normal':\n",
        "                init.normal_(m.weight.data, 0.0, init_gain)\n",
        "            elif init_type == 'xavier':\n",
        "                init.xavier_normal_(m.weight.data, gain=init_gain)\n",
        "            elif init_type == 'kaiming':\n",
        "                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "            elif init_type == 'orthogonal':\n",
        "                init.orthogonal_(m.weight.data, gain=init_gain)\n",
        "            else:\n",
        "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
        "            if hasattr(m, 'bias') and m.bias is not None:\n",
        "                init.constant_(m.bias.data, 0.0)\n",
        "        elif classname.find('BatchNorm2d') != -1:  # BatchNorm Layer's weight is not a matrix; only normal distribution applies.\n",
        "            init.normal_(m.weight.data, 1.0, init_gain)\n",
        "            init.constant_(m.bias.data, 0.0)\n",
        "    print('initialize network with %s' % init_type)\n",
        "    net.apply(init_func)  # apply the initialization function <init_func>\n",
        "    \n",
        "init_weights(gtm)\n",
        "init_weights(mtg)\n",
        "init_weights(D_game)\n",
        "init_weights(D_movie)\n",
        "# init_weights(D_game_1x)\n",
        "# init_weights(D_movie_1x)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initialize network with normal\n",
            "initialize network with normal\n",
            "initialize network with normal\n",
            "initialize network with normal\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NocBu2d7YgJ"
      },
      "source": [
        "lr=0.0002\n",
        "beta1=0.5\n",
        "G_optimiser = torch.optim.Adam(itertools.chain(gtm.parameters(), mtg.parameters()), lr=lr, betas=(beta1, 0.999))\n",
        "D_optimiser = torch.optim.Adam(itertools.chain(D_game.parameters(), D_movie.parameters()), lr=lr/10, betas=(beta1, 0.999))\n",
        "# D_1x_optimiser = torch.optim.Adam(itertools.chain(D_game_1x.parameters(), D_movie_1x.parameters()), lr=lr/2, betas=(beta1, 0.999))\n",
        "criterion_idt = nn.L1Loss()\n",
        "criterion_cycle = nn.L1Loss()\n",
        "gan_loss = nn.MSELoss()\n",
        "# gan_loss = nn.BCEWithLogitsLoss()\n",
        "patch_loss = nn.BCELoss()\n",
        "# G_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(G_optimiser, mode='min', factor=0.2, threshold=0.01, patience=5)\n",
        "# D_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(D_optimiser, mode='min', factor=0.2, threshold=0.01, patience=5)\n",
        "# D_1x_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(D_1x_optimiser, mode='min', factor=0.2, threshold=0.01, patience=5)\n",
        "\n",
        "#define the training hyper parameters \n",
        "epoch = 0\n",
        "max_epoch = 50\n",
        "lambdaidt = 10\n",
        "lambda_gtm = 10\n",
        "lambda_mtg = 10"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rELLflnr8Opr"
      },
      "source": [
        "## 2.1.3 Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "grqsQvsIYrgB",
        "outputId": "d97bcdb7-10e8-49f6-89cd-550baabf94a1"
      },
      "source": [
        "wandb.init(project=\"GameToMovie\",name=\"r\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.8<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">resnet-higherGTMlambda</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/ccm/GameToMovie\" target=\"_blank\">https://wandb.ai/ccm/GameToMovie</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/ccm/GameToMovie/runs/353joz6b\" target=\"_blank\">https://wandb.ai/ccm/GameToMovie/runs/353joz6b</a><br/>\n",
              "                Run data is saved locally in <code>wandb/run-20210520_123848-353joz6b</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<h1>Run(353joz6b)</h1><p></p><iframe src=\"https://wandb.ai/ccm/GameToMovie/runs/353joz6b\" style=\"border:none;width:100%;height:400px\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f6408238390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aR5sZiQFeikU"
      },
      "source": [
        "real_label = torch.ones((batch_size,1,16,16),device=device)\n",
        "fake_label = torch.zeros((batch_size,1,16,16),device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IJ5s3dG7e3w"
      },
      "source": [
        "while epoch<max_epoch:\n",
        "    \n",
        "    for _ in range(50):\n",
        "        game_images = next(game_iter)\n",
        "        movie_images = next(movie_iter)\n",
        "        game_images = game_images.t`o(device)\n",
        "        movie_images = movie_images.to(device)\n",
        "        \n",
        "        fake_movie = gtm(game_images)\n",
        "        fake_game = mtg(movie_images)\n",
        "        cycle_game = mtg(fake_movie) #cycle back\n",
        "        cycle_movie = gtm(fake_game) #cycle back\n",
        "\n",
        "        # real data label is 1, fake data label is 0.\n",
        "        # real_label = torch.full((batch_size, 1), 1, device=device, dtype=torch.float32)\n",
        "        # fake_label = torch.full((batch_size, 1), 0, device=device, dtype=torch.float32)\n",
        "\n",
        "        #train gtm and mtg \n",
        "        #set the grads to zero\n",
        "        # real_label = Variable(Tensor(np.ones((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n",
        "        # fake_label = Variable(Tensor(np.zeros((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n",
        "        #generate movie from games and check Discriminator\n",
        "        for param in D_game.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in D_movie.parameters():\n",
        "            param.requires_grad = False\n",
        "        # for param in D_game_1x.parameters():\n",
        "        #     param.requires_grad = False\n",
        "        # for param in D_movie_1x.parameters():\n",
        "        #     param.requires_grad = False\n",
        "        G_optimiser.zero_grad()\n",
        "\n",
        "\n",
        "        # loss_gtm_1x = patch_loss(D_movie_1x(gen_movie),real_label_256)\n",
        "        # loss_mtg_1x = patch_loss(D_game_1x(gen_game),real_label_256)\n",
        "        loss_gtm = l2_loss(D_movie.discriminate(fake_movie),real_label)\n",
        "                #generate game from movies and check Disc\n",
        "        loss_mtg = l2_loss(D_game.discriminate(fake_game),real_label)        \n",
        "\n",
        "        \n",
        "        cycle_loss_GTM = criterion_cycle(cycle_game,game_images)*lambda_gtm\n",
        "        cycle_loss_MTG = criterion_cycle(cycle_movie,movie_images)*lambda_mtg\n",
        "\n",
        "        #identity loss\n",
        "        #passing a movie image to GameToMovie generator should give the identity\n",
        "        if lambdaidt>0:\n",
        "            idt_gtm = gtm(movie_images)\n",
        "            loss_idt_gtm = criterion_idt(idt_gtm,movie_images)*lambda_mtg*lambdaidt\n",
        "            idt_mtg = mtg(game_images)\n",
        "            loss_idt_mtg = criterion_idt(idt_mtg,game_images)*lambda_gtm*lambdaidt\n",
        "        else:\n",
        "            loss_idt_gtm = 0\n",
        "            loss_idt_mtg = 0\n",
        "     \n",
        "        loss_G = loss_gtm+loss_mtg+loss_idt_gtm+loss_idt_mtg+cycle_loss_GTM+cycle_loss_MTG#+loss_gtm_1x+loss_mtg_1x\n",
        "        loss_G.backward()\n",
        "        G_optimiser.step()\n",
        "        \n",
        "        \n",
        "        for param in D_game.parameters():\n",
        "            param.requires_grad = True\n",
        "        for param in D_movie.parameters():\n",
        "            param.requires_grad = True     \n",
        "        # for param in D_game_1x.parameters():\n",
        "        #     param.requires_grad = True\n",
        "        # for param in D_movie_1x.parameters():\n",
        "        #     param.requires_grad = True  \n",
        "        D_optimiser.zero_grad()\n",
        "        \n",
        "        loss_real_game = l2_loss(D_game.discriminate(game_images),real_label)\n",
        "        loss_real_movie = l2_loss(D_movie.discriminate(movie_images),real_label)\n",
        "\n",
        "        loss_fake_game = l2_loss(D_game.discriminate(fake_game.detach()),fake_label)\n",
        "        loss_fake_movie = l2_loss(D_movie.discriminate(fake_movie.detach()),fake_label)\n",
        "        \n",
        "        # loss_real_game_1x = patch_loss(D_game_1x(game_images),real_label)\n",
        "        # loss_real_movie_1x = patch_loss(D_movie_1x(movie_images),real_label)\n",
        "        # loss_fake_game_1x = patch_loss(D_game_1x(gen_game),fake_label)\n",
        "        # loss_fake_movie_1x = patch_loss(D_movie_1x(gen_movie),fake_label)\n",
        "        \n",
        "        loss_D_game = (loss_real_game+loss_fake_game)*0.5\n",
        "        loss_D_movie = (loss_real_movie+loss_fake_movie)*0.5\n",
        "        \n",
        "        # loss_D_game_1x = (loss_real_game_1x+loss_fake_game_1x)*0.5\n",
        "        # loss_D_movie_1x = (loss_real_movie_1x+loss_fake_movie_1x)*0.5\n",
        "       \n",
        "        loss_D_game.backward()\n",
        "        loss_D_movie.backward()\n",
        "        # loss_D_game_1x.backward()\n",
        "        # loss_D_movie_1x.backward()\n",
        "\n",
        "        D_optimiser.step()\n",
        "    wandb.log({ \"Generator Loss\":loss_G.item(),\"Game Discriminator Loss\":loss_D_game.item(),\"Movie Discriminator Loss\":loss_D_movie.item(),\"Cycle Loss\":(cycle_loss_GTM+cycle_loss_MTG).item(),\n",
        "               \"Identity Loss\":(loss_idt_gtm+loss_idt_mtg).item(),\"Gen Learning Rate\":G_optimiser.param_groups[0]['lr'],\"Disc Learning Rate\":D_optimiser.param_groups[0]['lr']})\n",
        "    epoch+=1\n",
        "    G_scheduler.step(0)\n",
        "    D_scheduler.step(0)\n",
        "    # D_1x_scheduler.step(0)\n",
        "    plt.figure(figsize = (30,30))\n",
        "    imgList = []\n",
        "    for i in range(4):\n",
        "        imgList.extend([game_images[i],idt_mtg[i],fake_movie[i],cycle_game[i],movie_images[i],idt_gtm[i],fake_game[i],cycle_movie[i]])\n",
        "    grid_img = torchvision.utils.make_grid(imgList, nrow=4)\n",
        "    plt.imshow(grid_img.cpu().data.permute(0,2,1).contiguous().permute(2,1,0))\n",
        "    name =\"images/epoch\"+\"_\"+str(epoch)+\".png\"\n",
        "    plt.savefig(name)\n",
        "    wandb.save(name)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6BsDdSA7h-g"
      },
      "source": [
        "def save_models(epoch):\n",
        "    torch.save(D_game,\"disc_game{}.h5\".format(epoch))\n",
        "    torch.save(D_movie,\"disc_movie{}.h5\".format(epoch))\n",
        "    torch.save(gtm,\"gtmGen{}.h5\".format(epoch))\n",
        "    torch.save(mtg,\"mtgGen{}.h5\".format(epoch))\n",
        "    wandb.save(\"disc_game{}.h5\".format(epoch))\n",
        "    wandb.save(\"disc_movie{}.h5\".format(epoch))\n",
        "    wandb.save(\"gtmGen{}.h5\".format(epoch))\n",
        "    wandb.save(\"mtgGen{}.h5\".format(epoch))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEd2tRGr7a75"
      },
      "source": [
        "def cycle(iterable):\n",
        "    while True:\n",
        "        for x in iterable:\n",
        "            yield x\n",
        "game_iter = cycle(game_train_loader)\n",
        "movie_iter = cycle(movie_train_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aokji6Y3ZaJA"
      },
      "source": [
        "# 3 Face Model\n",
        "For this part we use the already extracted frames from the movies instead of going through the original videos. We perform this in the original resolution and then resize the images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n7Tq7krjxa-"
      },
      "source": [
        "## 3.1 Face Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVKdJNF8tQfh"
      },
      "source": [
        "from facenet_pytorch import MTCNN\n",
        "mtcnn = MTCNN(keep_all=False, image_size=256, margin=50,post_process=True,min_face_size=100,device=device)\n",
        "def getFaces(images,path,out_path):\n",
        "    Path(out_path).mkdir(parents=True, exist_ok=True)\n",
        "    for image in images:\n",
        "        filename = path+image\n",
        "        new_name = out_path+image\n",
        "        im = Image.open(filename)\n",
        "        save_paths = new_name\n",
        "        mtcnn(im, save_path=save_paths)\n",
        "    print(\"All Faces Extracted\")\n",
        "getFaces(game_images,game_path,\"/content/drive/MyDrive/Game-Movie/face_images/game/\")\n",
        "getFaces(movie_images,movies_path,\"/content/drive/MyDrive/Game-Movie/face_images/movies/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_CVv1O5trB_"
      },
      "source": [
        "Convert the dataset to h5 format for faster IO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_tTJseutd4x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9f1e6da-a267-4a88-d33c-924636ece0f1"
      },
      "source": [
        "game_path = \"/content/drive/MyDrive/Game-Movie/face_images/game/\"\n",
        "movie_path = \"/content/drive/MyDrive/Game-Movie/face_images/movies/\"\n",
        "game_fileName = \"/content/drive/MyDrive/Game-Movie/face_images/h5/game_faces.h5\"\n",
        "movie_fileName = \"/content/drive/MyDrive/Game-Movie/face_images/h5/movie_faces.h5\"\n",
        "def converth5(path,output_path):\n",
        "    #get names of the images and randomize\n",
        "    # Path(output_path).mkdir(parents=True, exist_ok=True)\n",
        "    files = [name for name in os.listdir(path)]\n",
        "    random.shuffle(files)#inplace operation\n",
        "    #define some variable according to the size\n",
        "    size = 256\n",
        "    numTrain = int(0.15*len(files))\n",
        "    numTest = len(files)-numTrain\n",
        "    #create the h5 files\n",
        "    f = h5py.File(output_path, \"w\")\n",
        "    with f as out:\n",
        "        out.create_dataset(\"X_train\",(numTrain,size,size,3),dtype='u1')   \n",
        "        out.create_dataset(\"X_test\",(numTest,size,size,3),dtype='u1')\n",
        "        #populate the h5 file\n",
        "    f = h5py.File(output_path, \"a\")\n",
        "    with f as out:\n",
        "        for index,img_name in enumerate(files[:numTrain]):\n",
        "            img = Image.open(path+img_name)      \n",
        "            out['X_train'][index] = np.asarray(img)\n",
        "        for index,img_name in enumerate(files[numTrain:]):\n",
        "            img = Image.open(path+img_name)      \n",
        "            out['X_test'][index] = np.asarray(img)\n",
        "    print(\"Conversion Complete\")\n",
        "converth5(game_path,game_fileName)\n",
        "converth5(movie_path,movie_fileName)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Conversion Complete\n",
            "Conversion Complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8fwEdBXV-rw"
      },
      "source": [
        "## Dataset and Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElVKCgl5rXL2"
      },
      "source": [
        "class dataset_h5(Dataset):\n",
        "    def __init__(self, in_file, transform=None,train=True):\n",
        "        super(dataset_h5, self).__init__()\n",
        "        self.train = train\n",
        "        self.file = h5py.File(in_file, 'r')\n",
        "        self.transform = transform\n",
        " \n",
        "    def __getitem__(self, index):\n",
        "        if self.train:\n",
        "            x = self.file['X_train'][index]\n",
        "        else:\n",
        "            x = self.file['X_test'][index]\n",
        "        # Preprocessing each image\n",
        "        if self.transform is not None:\n",
        "            x = self.transform(x)   \n",
        "        return x #x is original and p is inpainted\n",
        " \n",
        "    def __len__(self):\n",
        "        if self.train:\n",
        "            return self.file['X_train'].shape[0]\n",
        "        else:\n",
        "            return self.file['X_test'].shape[0]\n",
        "\n",
        "img_width = 256\n",
        "game_fileName = \"/content/drive/MyDrive/Game-Movie/face_images/h5/game_faces.h5\"\n",
        "movie_fileName = \"/content/drive/MyDrive/Game-Movie/face_images/h5/movie_faces.h5\"\n",
        "img_transform = transforms.Compose([transforms.ToTensor(),transforms.RandomRotation(45),transforms.RandomHorizontalFlip(p=0.4)])\n",
        "test_transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "game_trainset = dataset_h5(game_fileName,transform=img_transform)\n",
        "movie_trainset = dataset_h5(movie_fileName,transform=img_transform)\n",
        "\n",
        "game_testset = dataset_h5(game_fileName,transform=test_transform,train=False)\n",
        "movie_testset = dataset_h5(movie_fileName,transform=test_transform,train=False)\n",
        "Path(\"outputs/\").mkdir(parents=True, exist_ok=True) #generate path to save produced images\n",
        "\n",
        "batch_size=4\n",
        "\n",
        "game_train_loader = DataLoader(game_trainset,batch_size=batch_size, shuffle=True, num_workers=0,drop_last=True)\n",
        "movie_train_loader = DataLoader(movie_trainset,batch_size=batch_size, shuffle=True, num_workers=0,drop_last=True)\n",
        "\n",
        "game_test_loader = DataLoader(game_testset,batch_size=batch_size, shuffle=True, num_workers=0,drop_last=True)\n",
        "movie_test_loader = DataLoader(movie_testset,batch_size=batch_size, shuffle=True, num_workers=0,drop_last=True)\n",
        "\n",
        "real_label = torch.ones((batch_size,1,16,16),device=device)\n",
        "fake_label = torch.zeros((batch_size,1,16,16),device=device)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZjY_eIKsZne"
      },
      "source": [
        "def cycle(iterable):\n",
        "    while True:\n",
        "        for x in iterable:\n",
        "            yield x\n",
        "game_iter = cycle(game_train_loader)\n",
        "movie_iter = cycle(movie_train_loader)\n",
        "\n",
        "game_test_iter = cycle(game_test_loader)\n",
        "movie_test_iter = cycle(movie_test_loader)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMpAh5AassFV"
      },
      "source": [
        "wandb.init(project=\"GameToMovie\",resume=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yThytxpPwsxx"
      },
      "source": [
        "code to load model an continue training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqCETf2Vv3Ax"
      },
      "source": [
        "run_path=\"23w33gvf\"\n",
        "epoch=50\n",
        "D_game = wandb.restore(\"disc_game{}.h5\".format(epoch),run_path=\"ccm/GameToMovie/{}\".format(run_path))\n",
        "D_movie = wandb.restore(\"disc_movie{}.h5\".format(epoch),run_path=\"ccm/GameToMovie/{}\".format(run_path))\n",
        "gtm = wandb.restore(\"gtmGen{}.h5\".format(epoch),run_path=\"ccm/GameToMovie/{}\".format(run_path))\n",
        "mtg = wandb.restore(\"mtgGen{}.h5\".format(epoch),run_path=\"ccm/GameToMovie/{}\".format(run_path))\n",
        "D_game = torch.load(D_game.name).to(device)\n",
        "D_movie = torch.load(D_movie.name).to(device)\n",
        "gtm = torch.load(gtm.name).to(device)\n",
        "mtg = torch.load(mtg.name).to(device)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XilHrwhis0PQ"
      },
      "source": [
        "while epoch<50:\n",
        "    \n",
        "    for _ in range(75):\n",
        "        game_images = next(game_iter)\n",
        "        movie_images = next(movie_iter)\n",
        "        game_images = game_images.to(device)\n",
        "        movie_images = movie_images.to(device)\n",
        "        \n",
        "        fake_movie = gtm(game_images)\n",
        "        fake_game = mtg(movie_images)\n",
        "        cycle_game = mtg(fake_movie) #cycle back\n",
        "        cycle_movie = gtm(fake_game) #cycle back\n",
        "\n",
        "        # real data label is 1, fake data label is 0.\n",
        "        #set the grads to zero\n",
        "        #generate movie from games and check Discriminator\n",
        "        for param in D_game.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in D_movie.parameters():\n",
        "            param.requires_grad = False\n",
        "        # for param in D_game_1x.parameters():\n",
        "        #     param.requires_grad = False\n",
        "        # for param in D_movie_1x.parameters():\n",
        "        #     param.requires_grad = False\n",
        "        G_optimiser.zero_grad()\n",
        "\n",
        "\n",
        "        # loss_gtm_1x = patch_loss(D_movie_1x(fake_movie),real_label_256)\n",
        "        # loss_mtg_1x = patch_loss(D_game_1x(fake_game),real_label_256)\n",
        "        loss_gtm = gan_loss(D_movie(fake_movie),real_label)\n",
        "                #generate game from movies and check Disc\n",
        "        loss_mtg = gan_loss(D_game(fake_game),real_label)        \n",
        "\n",
        "        \n",
        "        cycle_loss_GTM = criterion_cycle(cycle_game,game_images)*lambda_gtm\n",
        "        cycle_loss_MTG = criterion_cycle(cycle_movie,movie_images)*lambda_mtg\n",
        "\n",
        "        #identity loss\n",
        "        #passing a movie image to GameToMovie generator should give the identity\n",
        "        if lambdaidt>0:\n",
        "            idt_gtm = gtm(movie_images)\n",
        "            loss_idt_gtm = criterion_idt(idt_gtm,movie_images)*lambdaidt\n",
        "            idt_mtg = mtg(game_images)\n",
        "            loss_idt_mtg = criterion_idt(idt_mtg,game_images)*lambdaidt\n",
        "        else:\n",
        "            loss_idt_gtm = 0\n",
        "            loss_idt_mtg = 0\n",
        "     \n",
        "        loss_G = loss_gtm+loss_mtg+loss_idt_gtm+loss_idt_mtg+cycle_loss_GTM+cycle_loss_MTG#+loss_gtm_1x+loss_mtg_1x\n",
        "        loss_G.backward()\n",
        "        G_optimiser.step()\n",
        "        \n",
        "        \n",
        "        for param in D_game.parameters():\n",
        "            param.requires_grad = True\n",
        "        for param in D_movie.parameters():\n",
        "            param.requires_grad = True     \n",
        "        # for param in D_game_1x.parameters():\n",
        "        #     param.requires_grad = True\n",
        "        # for param in D_movie_1x.parameters():\n",
        "        #     param.requires_grad = True  \n",
        "        D_optimiser.zero_grad()\n",
        "        \n",
        "        loss_real_game = gan_loss(D_game(game_images.detach()),real_label)\n",
        "        loss_real_movie = gan_loss(D_movie(movie_images.detach()),real_label)\n",
        "\n",
        "        loss_fake_game = gan_loss(D_game(fake_game.detach()),fake_label)\n",
        "        loss_fake_movie = gan_loss(D_movie(fake_movie.detach()),fake_label)\n",
        "        \n",
        "        # loss_real_game_1x = patch_loss(D_game_1x(game_images),real_label_256)\n",
        "        # loss_real_movie_1x = patch_loss(D_movie_1x(movie_images),real_label_256)\n",
        "        # loss_fake_game_1x = patch_loss(D_game_1x(fake_game),fake_label_256)\n",
        "        # loss_fake_movie_1x = patch_loss(D_movie_1x(fake_movie),fake_label_256)\n",
        "        \n",
        "        loss_D_game = (loss_real_game+loss_fake_game)*0.5\n",
        "        loss_D_movie = (loss_real_movie+loss_fake_movie)*0.5\n",
        "        \n",
        "        # loss_D_game_1x = (loss_real_game_1x+loss_fake_game_1x)*0.5\n",
        "        # loss_D_movie_1x = (loss_real_movie_1x+loss_fake_movie_1x)*0.5\n",
        "       \n",
        "        loss_D_game.backward()\n",
        "        loss_D_movie.backward()\n",
        "        # loss_D_game_1x.backward()\n",
        "        # loss_D_movie_1x.backward()\n",
        "\n",
        "        D_optimiser.step()\n",
        "    wandb.log({ \"Generator Loss\":loss_G.item(),\"Game Discriminator Loss\":loss_D_game.item(),\"Movie Discriminator Loss\":loss_D_movie.item(),\"Cycle Loss\":(cycle_loss_GTM+cycle_loss_MTG).item(),\n",
        "               \"Identity Loss\":(loss_idt_gtm+loss_idt_mtg).item()})\n",
        "    #,\"Game Pixel Loss\":loss_D_game_1x.item(),\"Movie Pixel Loss\":loss_D_movie_1x.item()})\n",
        "    epoch+=1\n",
        "    # G_scheduler.step(0)\n",
        "    # D_scheduler.step(0)\n",
        "    # D_1x_scheduler.step(0)\n",
        "    if epoch % 5 ==0:\n",
        "        plt.figure(figsize = (30,30))\n",
        "        imgList = []\n",
        "        for i in range(batch_size):\n",
        "            imgList.extend([game_images[i],idt_mtg[i],fake_movie[i],cycle_game[i],movie_images[i],idt_gtm[i],fake_game[i],cycle_movie[i]])\n",
        "        grid_img = torchvision.utils.make_grid(imgList, nrow=4)\n",
        "        plt.imshow(grid_img.cpu().data.permute(0,2,1).contiguous().permute(2,1,0))\n",
        "        name =\"images/epoch\"+\"_\"+str(epoch)+\".png\"\n",
        "        save_models(epoch)\n",
        "        plt.savefig(name)\n",
        "        wandb.save(name)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_d3mkGxqs0P7"
      },
      "source": [
        "torch.save(gtm.state_dict(),\"/content/drive/MyDrive/Game-Movie/Models/game_to_movieGen.pt\")\n",
        "torch.save(mtg.state_dict(),\"/content/drive/MyDrive/Game-Movie/Models/movie_to_gameGen.pt\")\n",
        "torch.save(D_game.state_dict(),\"/content/drive/MyDrive/Game-Movie/Models/D_game.pt\")\n",
        "torch.save(D_movie.state_dict(),\"/content/drive/MyDrive/Game-Movie/Models/D_movie.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aIaBQT3u-SE"
      },
      "source": [
        "gtm.load_state_dict(torch.load(\"/content/drive/MyDrive/Game-Movie/Models/game_to_movieGen.pt\"))\n",
        "mtg.load_state_dict(torch.load(\"/content/drive/MyDrive/Game-Movie/Models/movie_to_gameGen.pt\"))\n",
        "D_game.load_state_dict(torch.load(\"/content/drive/MyDrive/Game-Movie/Models/D_game.pt\"))\n",
        "D_movie.load_state_dict(torch.load(\"/content/drive/MyDrive/Game-Movie/Models/D_movie.pt\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGRtgm2YJV_p"
      },
      "source": [
        "function to generate results and images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ypglfs8JVmy"
      },
      "source": [
        "with torch.no_grad():\n",
        "    game_images = next(game_test_iter)\n",
        "    movie_images = next(movie_test_iter)\n",
        "    game_images = game_images.to(device)\n",
        "    movie_images = movie_images.to(device)\n",
        "    fake_movie = gtm(game_images)\n",
        "    fake_game = mtg(movie_images.detach())\n",
        "    idt_game = mtg(game_images.detach())\n",
        "    idt_movie = gtm(movie_images.detach())\n",
        "    cycle_game = mtg(fake_movie.detach())\n",
        "    cycle_movie = gtm(fake_game.detach())\n",
        "\n",
        "    imgList = []\n",
        "    plt.figure(figsize = (30,30))\n",
        "    for i in range(4):\n",
        "        imgList.extend([game_images[i],idt_game[i],fake_movie[i],cycle_game[i],movie_images[i],idt_movie[i],fake_game[i],cycle_movie[i]])\n",
        "    grid_img = torchvision.utils.make_grid(imgList, nrow=4)\n",
        "    plt.imshow(grid_img.cpu().data.permute(0,2,1).contiguous().permute(2,1,0))\n",
        "    name =\"images/FINAL6.png\"\n",
        "    plt.savefig(name)\n",
        "    wandb.save(name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T64MpSRsNPSJ"
      },
      "source": [
        "# 4 Real World Application"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lx3ZF_H11zA8"
      },
      "source": [
        "tr = transforms.ToTensor()\n",
        "untr = transforms.ToPILImage()\n",
        "!pip install facenet-pytorch\n",
        "from facenet_pytorch import MTCNN\n",
        "mtcnn = MTCNN(keep_all=False, image_size=256, margin=50,post_process=True,min_face_size=100,device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IACSbc7_LCmB"
      },
      "source": [
        "import time "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdM8uDgLNT7L"
      },
      "source": [
        "count=0\n",
        "start = 270*30 #start at 4:30\n",
        "max_count = start + 300 #10 seconds at 30 fps\n",
        "\n",
        "save_loc = \"/content/drive/MyDrive/Game-Movie/FinalOut/\"\n",
        "Path(save_loc).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "video = cv2.VideoCapture(game_file)   # capturing the video from the given path\n",
        "frameRate = int(video.get(cv2.CAP_PROP_FPS)) #frame rate\n",
        "\n",
        "\n",
        "while (video.isOpened()): \n",
        "    ret, frame = video.read()\n",
        "    count+=1\n",
        "    if (ret!=True):\n",
        "        break\n",
        "    if count>=start: \n",
        "        \n",
        "        im_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "        im = im_pil.crop((108, 0, 1173, 600)) \n",
        "\n",
        "\n",
        "        box,_ = mtcnn.detect(im)\n",
        "        cropped = im.crop(box[0].round())\n",
        "        resized = cropped.resize((256,256))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            translated_im = gtm(tr(resized).unsqueeze_(0).to(device))\n",
        "            img = plt.imshow(translated_im.squeeze().permute(1,2,0).cpu())\n",
        "            plt.axis(\"off\")\n",
        "            new_file = save_loc+\"_\"+str(count)+\".jpg\"\n",
        "            plt.savefig(new_file, bbox_inches='tight')\n",
        "\n",
        "\n",
        "            # final = untr(translated_im.squeeze())\n",
        "        # final.save(new_file)\n",
        "\n",
        "    if count==max_count:\n",
        "        break\n",
        "video.release()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}